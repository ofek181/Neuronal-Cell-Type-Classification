{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm,colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ophir\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from lspin import Model\n",
    "from lspin import convertToOneHot, DataSet_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath('')\n",
    "inhibitory = dir_path + '/data/dataframe/mouse/inhibitory_transgenic_data.csv'\n",
    "excitatory = dir_path + '/data/dataframe/mouse/excitatory_transgenic_data.csv'\n",
    "data_inhibitory = pd.read_csv(inhibitory)\n",
    "data_excitatory = pd.read_csv(excitatory)\n",
    "results_mouse = dir_path + '/results/MLP/transgenic/mouse'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame, n_classes: int) -> tuple:\n",
    "    \"\"\"\n",
    "    :param df: raw dataframe to be processed.\n",
    "    :param n_classes: number of classes in the dataframe.\n",
    "    :return: split into train, validation and test.\n",
    "    \"\"\"\n",
    "    db = df.dropna(axis=1, how='all')\n",
    "    db = db.dropna(axis=0)\n",
    "    irrelevant_columns = ['dendrite_type', 'layer', 'mean_clipped', 'file_name', 'mean_threshold_index',                           'mean_peak_index', 'mean_trough_index', 'mean_upstroke_index',                                           'mean_downstroke_index', 'mean_fast_trough_index']\n",
    "    db = db.drop([x for x in irrelevant_columns if x in df.columns], axis=1)\n",
    "    db['transgenic_line'] = pd.Categorical(db['transgenic_line'])\n",
    "    class_names = dict(enumerate(db['transgenic_line'].cat.categories))\n",
    "    db['transgenic_line'] = db['transgenic_line'].cat.codes\n",
    "    scaler = StandardScaler()\n",
    "    y = db.pop('transgenic_line')\n",
    "    y = y.values.astype(np.float32)\n",
    "    y = to_categorical(y, num_classes=n_classes)\n",
    "    x = db.values.astype(np.float32)\n",
    "    x = scaler.fit_transform(x)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.9, random_state=42)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.65, random_state=42)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = preprocess_data(data_inhibitory, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data = DataSet_meta(**{'_data':x_train, '_labels':y_train,'_meta':y_train,\n",
    "                       '_valid_data':x_val, '_valid_labels':y_val,'_valid_meta':y_val,\n",
    "                       '_test_data':x_test, '_test_labels':y_test,'_test_meta':y_test})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(356, 20)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(356, 5)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(62, 20)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(62, 5)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(193, 20)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(193, 5)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 1., 0., 0.], dtype=float32)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[8]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run LSPIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hyper-parameter specification\n",
    "model_params = {\"input_node\": x_train.shape[1], \"hidden_layers_node\": [128, 64, 32, 16], \"output_node\": 5,\n",
    "                \"feature_selection\": True, \"gating_net_hidden_layers_node\": [10], \"display_step\": 1000,\n",
    "                'activation_pred': 'l_relu', 'activation_gating': 'tanh'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_params = {'batch_size':x_train.shape[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# objective function for optuna hyper-parameter optimization\n",
    "def objective(trial):\n",
    "    global model\n",
    "    \n",
    "    # hyper-parameter to optimize: lambda, learning rate, number of epochs\n",
    "    model_params['lam'] = trial.suggest_loguniform('lam', 1e-3, 1e-2)\n",
    "    training_params['lr'] = trial.suggest_loguniform('learning_rate', 1e-2, 2e-1)\n",
    "    training_params['num_epoch'] = trial.suggest_categorical('num_epoch', [2000, 5000, 10000, 15000])\n",
    "\n",
    "    # specify the model with these parameters and train the model\n",
    "    model = Model(**model_params)\n",
    "    train_acces, train_losses, val_acces, val_losses = model.train(dataset=data, **training_params)\n",
    "\n",
    "    print(\"In trial:---------------------\")\n",
    "    y_pred = model.test(x_val)\n",
    "    true = np.argmax(y_val, axis=1)\n",
    "    accuracy = accuracy_score(true, y_pred)\n",
    "    return accuracy\n",
    "        \n",
    "def callback(study, trial):\n",
    "    global best_model\n",
    "    if study.best_trial == trial:\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-08-21 17:42:51,194]\u001B[0m A new study created in memory with name: lspin\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ophir\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ophir\\anaconda3\\envs\\thesis\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MSc\\thesis\\Code\\lspin.py:314: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n",
      "C:\\MSc\\thesis\\Code\\lspin.py:345: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples : 356\n",
      "Epoch: 1000 train loss=0.093911693 valid loss= 1.394084215 valid acc= 0.629032254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-08-21 17:43:03,224]\u001B[0m Trial 0 finished with value: 0.7096774193548387 and parameters: {'lam': 0.0012934518594018374, 'learning_rate': 0.08294597552393042, 'num_epoch': 2000}. Best is trial 0 with value: 0.7096774193548387.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.012249043 valid loss= 1.741840363 valid acc= 0.709677398\n",
      "Optimization Finished!\n",
      "test loss: 2.3634254932403564, test acc: 0.6269429922103882\n",
      "In trial:---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MSc\\thesis\\Code\\lspin.py:314: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n",
      "C:\\MSc\\thesis\\Code\\lspin.py:345: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples : 356\n",
      "Epoch: 1000 train loss=0.762933791 valid loss= 0.761717319 valid acc= 0.725806475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-08-21 17:43:13,973]\u001B[0m Trial 1 finished with value: 0.7580645161290323 and parameters: {'lam': 0.0014532955192059103, 'learning_rate': 0.011044116394209883, 'num_epoch': 2000}. Best is trial 1 with value: 0.7580645161290323.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.470199257 valid loss= 0.693689644 valid acc= 0.758064508\n",
      "Optimization Finished!\n",
      "test loss: 0.9208469986915588, test acc: 0.6891191601753235\n",
      "In trial:---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MSc\\thesis\\Code\\lspin.py:314: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n",
      "C:\\MSc\\thesis\\Code\\lspin.py:345: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples : 356\n",
      "Epoch: 1000 train loss=0.199875966 valid loss= 0.857283056 valid acc= 0.725806475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-08-21 17:43:25,099]\u001B[0m Trial 2 finished with value: 0.7258064516129032 and parameters: {'lam': 0.009161125671220667, 'learning_rate': 0.03880933574449226, 'num_epoch': 2000}. Best is trial 1 with value: 0.7580645161290323.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.083770342 valid loss= 0.999063849 valid acc= 0.725806475\n",
      "Optimization Finished!\n",
      "test loss: 1.578796625137329, test acc: 0.6632124185562134\n",
      "In trial:---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MSc\\thesis\\Code\\lspin.py:314: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n",
      "C:\\MSc\\thesis\\Code\\lspin.py:345: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  prev_x = tf.layers.batch_normalization(prev_x, training=is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples : 356\n",
      "Epoch: 1000 train loss=0.639404714 valid loss= 0.679481626 valid acc= 0.790322602\n",
      "Epoch: 2000 train loss=0.171460435 valid loss= 0.778588474 valid acc= 0.741935492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15948\\3314008047.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mstudy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptuna\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreate_study\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstudy_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'lspin'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdirection\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'maximize'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mstudy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mobjective\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_trials\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcallback\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\optuna\\study\\study.py\u001B[0m in \u001B[0;36moptimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    407\u001B[0m             \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    408\u001B[0m             \u001B[0mgc_after_trial\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgc_after_trial\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 409\u001B[1;33m             \u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshow_progress_bar\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    410\u001B[0m         )\n\u001B[0;32m    411\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     74\u001B[0m                 \u001B[0mreseed_sampler_rng\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m                 \u001B[0mtime_start\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 76\u001B[1;33m                 \u001B[0mprogress_bar\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprogress_bar\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     77\u001B[0m             )\n\u001B[0;32m     78\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m             \u001B[0mtrial\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_run_trial\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstudy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    164\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m             \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\thesis\\lib\\site-packages\\optuna\\study\\_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    211\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    212\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 213\u001B[1;33m         \u001B[0mvalue_or_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    214\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mexceptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrialPruned\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    215\u001B[0m         \u001B[1;31m# TODO(mamu): Handle multi-objective cases.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15948\\4254100094.py\u001B[0m in \u001B[0;36mobjective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;31m# specify the model with these parameters and train the model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mmodel_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0mtrain_acces\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_losses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_acces\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_losses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mtraining_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"In trial:---------------------\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\MSc\\thesis\\Code\\lspin.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, dataset, batch_size, num_epoch, lr, compute_sim)\u001B[0m\n\u001B[0;32m    603\u001B[0m             \u001B[1;31m# Loop over all batches\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    604\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtotal_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 605\u001B[1;33m                 \u001B[0mbatch_xs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_ys\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_zs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnext_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    606\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    607\u001B[0m                 \u001B[1;31m# for the 2nd penalty, precompute the K matrix\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\MSc\\thesis\\Code\\lspin.py\u001B[0m in \u001B[0;36mnext_batch\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m    146\u001B[0m             \u001B[0mremaining\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[1;33m-\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_samples\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    147\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_shuffle\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 148\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_shuffle_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    149\u001B[0m             \u001B[0mstart\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    150\u001B[0m             data_batch = np.concatenate([data_batch, self._data[:remaining]],\n",
      "\u001B[1;32mC:\\MSc\\thesis\\Code\\lspin.py\u001B[0m in \u001B[0;36m_shuffle_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    131\u001B[0m         \u001B[0mshuffled_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_samples\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m         \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mshuffled_idx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 133\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mshuffled_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    134\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_labeled\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    135\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_labels\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mshuffled_idx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# optimize the model via Optuna and obtain the best model with smallest validation mse\n",
    "best_model = None\n",
    "model = None\n",
    "study = optuna.create_study(study_name='lspin', direction='maximize')\n",
    "study.optimize(func=objective, n_trials=10, n_jobs=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the training gate matrix\n",
    "gate_mat_train = best_model.get_prob_alpha(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_lr = study.best_params['learning_rate']\n",
    "best_epoch = study.best_params['num_epoch']\n",
    "best_lam = study.best_params['lam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test the best model\n",
    "y_pred = best_model.test(x_test)\n",
    "            \n",
    "print(\"Trial Finished*************\")\n",
    "print(\"Best model's lambda: {}\".format(best_lam))\n",
    "print(\"Best model's learning rate: {}\".format(best_lr))\n",
    "print(\"Best model's num of epochs: {}\".format(best_epoch))\n",
    "print(\"Test accuracy : {}\".format(accuracy_score(np.argmax(y_test, axis=1), y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y_pred[1])\n",
    "np.argmax(y_test[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Comparing the training gates to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cmap = cm.Blues\n",
    "# bounds=[0,0.5,1]\n",
    "# norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "#\n",
    "# title_size = 30\n",
    "# xtick_size = 20\n",
    "# ytick_size = 20\n",
    "# xlabel_size = 35\n",
    "# ylabel_size = 35\n",
    "# colorbar_tick_size = 20\n",
    "# title_pad = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 2,sharex=False, sharey=True,figsize=(8, 6))\n",
    "#\n",
    "# sorted_order = np.concatenate((np.where(train_label == 1)[0],np.where(train_label == 2)[0]))\n",
    "#\n",
    "# im1 = axes[0].imshow(ref_feat_mat_train[sorted_order,:].astype(int),aspect='auto',cmap=cmap, norm=norm)\n",
    "# axes[0].set_title(\"Ground Truth\",fontsize=title_size,fontweight=\"bold\",pad=title_pad)\n",
    "# axes[0].set_ylabel(\"Sample Index\",fontsize=ylabel_size)\n",
    "# axes[0].set_yticks([1,3,5,7,9])\n",
    "# axes[0].set_yticklabels([2,4,6,8,10],fontsize=ytick_size)\n",
    "# axes[0].set_xticks(list(range(5)))\n",
    "# axes[0].set_xticklabels(list(range(1,6)),fontsize=xtick_size)\n",
    "# axes[0].set_xlabel(\"Feature Index\",fontsize=xlabel_size,labelpad=-5)\n",
    "#\n",
    "# cbar = fig.colorbar(im1,ax=axes[0], cmap=cmap, norm=norm, boundaries=bounds, ticks=[0, 1])\n",
    "# cbar.ax.tick_params(labelsize=colorbar_tick_size)\n",
    "#\n",
    "# im2 = axes[1].imshow(gate_mat_train[sorted_order,:],aspect='auto',cmap=cmap)\n",
    "# axes[1].set_title(\"LLSPIN Gates\",fontsize=title_size,fontweight=\"bold\",pad=title_pad)\n",
    "# axes[1].set_yticks([1,3,5,7,9])\n",
    "# axes[1].set_yticklabels([2,4,6,8,10],fontsize=ytick_size)\n",
    "# axes[1].set_xticks(list(range(5)))\n",
    "# axes[1].set_xticklabels(list(range(1,6)),fontsize=xtick_size)\n",
    "# axes[1].set_xlabel(\"Feature Index\",fontsize=xlabel_size,labelpad=-5)\n",
    "#\n",
    "# cbar = fig.colorbar(im2,ax=axes[1])\n",
    "# cbar.ax.tick_params(labelsize=colorbar_tick_size)\n",
    "#\n",
    "# plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}